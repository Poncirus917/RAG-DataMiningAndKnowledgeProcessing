import streamlit as st
from pymilvus import MilvusClient, connections, RRFRanker, AnnSearchRequest
from pymilvus.model.hybrid import BGEM3EmbeddingFunction
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
import torch

# ------------------ åˆå§‹åŒ–æ¨¡å— ------------------
@st.cache_resource
def init_components():
    """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ï¼ˆç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰"""
    # Milvusè¿æ¥
    client = MilvusClient("milvus.db")
    
    # åµŒå…¥æ¨¡å‹
    embedder = BGEM3EmbeddingFunction(
        model_name='BAAI/bge-m3',
        device='mps',
        use_fp16=False
    )
    
    # Qwenæ¨¡å‹
    model_name = "Qwen/Qwen3-0.6B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map="auto"
    )
    
    return client, embedder, tokenizer, model

# ------------------ RAGæœç´¢æ¨¡å— ------------------
def hybrid_search(_client, _embedder, query, limit=5):
    """æ··åˆæ£€ç´¢å‡½æ•°"""
    embeddings = _embedder.encode_documents([query])
    
    # å¤„ç†å¯†é›†å‘é‡
    dense_data = np.array(embeddings["dense"][0]).reshape(1, -1)
    
    # å¤„ç†ç¨€ç–å‘é‡
    csr_matrix = embeddings["sparse"][0].tocsr()
    if csr_matrix.ndim != 2:
        csr_matrix = csr_matrix.reshape(1, -1)
    
    # æ„å»ºæœç´¢è¯·æ±‚
    reqs = [
        AnnSearchRequest(
            data=dense_data,
            anns_field="dense_vector",
            param={"metric_type": "IP", "params": {"nprobe": 10}},
            limit=limit
        ),
        AnnSearchRequest(
            data=csr_matrix,
            anns_field="sparse_vector",
            param={"metric_type": "IP", "params": {"nprobe": 10}},
            limit=limit
        )
    ]
    
    # æ‰§è¡Œæ··åˆæœç´¢
    search_results = _client.hybrid_search(
        collection_name="text_collection",
        reqs=reqs,
        ranker=RRFRanker(k=60),
        output_fields=["text"]
    )
    
    return [hit['entity']['text'] for hit in search_results[0]]

# ------------------ LLMç”Ÿæˆæ¨¡å— ------------------
def generate_response(_tokenizer, _model, context, query):
    """ç”Ÿæˆå›ç­”"""
    SYSTEM_PROMPT = "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š"
    USER_PROMPT = f"""
    <context>
    {context}
    </context>
    <question>
    {query}
    </question>
    """
    
    # æ„å»ºè¾“å…¥
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": USER_PROMPT}
    ]
    text = _tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True
    )
    
    # ç”Ÿæˆå›ç­”
    inputs = _tokenizer([text], return_tensors="pt").to(_model.device)
    outputs = _model.generate(
        **inputs,
        max_new_tokens=512  # æ§åˆ¶ç”Ÿæˆé•¿åº¦
    )
    
    # è§£æè¾“å‡º
    response = _tokenizer.decode(
        outputs[0][len(inputs.input_ids[0]):],
        skip_special_tokens=True
    ).strip("\n")
    
    return response

# ------------------ Streamlitç•Œé¢ ------------------
st.title("RAG+Qwen æ™ºèƒ½é—®ç­”ç³»ç»Ÿ ğŸ’¬")

# åˆå§‹åŒ–ç»„ä»¶
try:
    client, embedder, tokenizer, model = init_components()
except Exception as e:
    st.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    st.stop()

# åˆå§‹åŒ–èŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘å°†ç»“åˆçŸ¥è¯†åº“ä¸ºæ‚¨è§£ç­”ã€‚"}]

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    
    # å¤„ç†å›ç­”
    with st.spinner("æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."):
        try:
            # RAGæ£€ç´¢
            context = hybrid_search(client, embedder, user_input)
            context_str = "\n".join(context[:3])  # å–å‰3æ¡ç»“æœ

            # --------- å†å²å›æº¯ä¼˜åŒ–ï¼šå°†å†å²å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†ä¼ å…¥ç”Ÿæˆæ¨¡å— ---------
            # å–æœ€è¿‘5è½®ï¼ˆç”¨æˆ·+åŠ©æ‰‹ï¼‰çš„æ¶ˆæ¯ï¼ˆå¯è°ƒæ•´ï¼‰
            recent_dialogs = st.session_state.messages[-10:]  # 5è½®å¯¹è¯ï¼Œç”¨æˆ·å’ŒåŠ©æ‰‹å„10æ¡æ¶ˆæ¯

            # æ‹¼æ¥å†å²å¯¹è¯æ–‡æœ¬ï¼Œæ ¼å¼ä¸ºè§’è‰²+å†…å®¹ï¼Œæ–¹ä¾¿æ¨¡å‹ç†è§£
            history_text = ""
            for msg in recent_dialogs:
                role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
                history_text += f"{role}: {msg['content']}\n"

            # å°†å†å²å¯¹è¯å’Œæ£€ç´¢åˆ°çš„çŸ¥è¯†åº“å†…å®¹æ‹¼æ¥ä¸ºæ–°çš„ä¸Šä¸‹æ–‡
            enhanced_context = context_str + "\n\nå†å²å¯¹è¯è®°å½•:\n" + history_text
            
            with st.spinner("ç”Ÿæˆå›ç­”ä¸­..."):
                response = generate_response(tokenizer, model, enhanced_context, user_input)
            # ----------------------------------------------------------------------

            # æ·»åŠ å¹¶æ˜¾ç¤ºå›ç­”
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.chat_message("assistant").write(response)
            
        except Exception as e:
            st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            st.session_state.messages.append({"role": "assistant", "content": f"å‡ºé”™äº†: {str(e)}"})
