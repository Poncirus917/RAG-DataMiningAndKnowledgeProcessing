import streamlit as st
from pymilvus import MilvusClient, connections, RRFRanker, AnnSearchRequest
from pymilvus.model.hybrid import BGEM3EmbeddingFunction
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
import torch

# ------------------ åˆå§‹åŒ–æ¨¡å— ------------------
@st.cache_resource
def init_components():
    """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ï¼ˆç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰"""
    # Milvusè¿æ¥
    client = MilvusClient("/home/prts/milvus_data/milvus.db")

    # åµŒå…¥æ¨¡å‹
    embedder = BGEM3EmbeddingFunction(
        model_name_or_path='/mnt/d/models/bge-m3',
        device='cuda',
        use_fp16=False
    )

    # Qwenæ¨¡å‹
    model_path = "/mnt/d/models/qwen3_0.6b"
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype="auto",
        device_map="auto"
    )

    return client, embedder, tokenizer, model

# ------------------ RAGæœç´¢æ¨¡å— ------------------
def hybrid_search(_client, _embedder, query, limit=5, alpha=0.7):
    """
    æ··åˆæ£€ç´¢ + æ–¹æ³•ä¸‰ rerankï¼šæ ‡é¢˜/æ‘˜è¦è¯­ä¹‰ç›¸ä¼¼åº¦åŠ æƒæ’åº
    alpha: åŸå§‹Milvuså¾—åˆ†å æ¯”ï¼Œ(1 - alpha)ä¸ºquery-summaryç›¸ä¼¼åº¦å æ¯”
    """
    # 1. è·å– query çš„å‘é‡è¡¨ç¤º
    embeddings = _embedder.encode_documents([query])
    dense_data = np.array(embeddings["dense"][0]).reshape(1, -1)
    csr_matrix = embeddings["sparse"][0].tocsr()
    if csr_matrix.ndim != 2:
        csr_matrix = csr_matrix.reshape(1, -1)

    # 2. æ„å»º Milvus hybrid search è¯·æ±‚
    reqs = [
        AnnSearchRequest(
            data=dense_data,
            anns_field="dense_vector",
            param={"metric_type": "IP", "params": {"nprobe": 10}},
            limit=limit
        ),
        AnnSearchRequest(
            data=csr_matrix,
            anns_field="sparse_vector",
            param={"metric_type": "IP", "params": {"nprobe": 10}},
            limit=limit
        )
    ]

    # 3. æ‰§è¡Œæ··åˆæœç´¢ï¼Œé»˜è®¤ä½¿ç”¨ RRF æ’åºå™¨
    search_results = _client.hybrid_search(
        collection_name="text_collection",
        reqs=reqs,
        ranker=RRFRanker(k=60),
        output_fields=["text"]
    )

    hits = search_results[0]
    texts = [hit.entity['text'] for hit in hits]

    # 4. è®¡ç®— query ä¸æ¯æ¡æ£€ç´¢ç»“æœæ‘˜è¦çš„è¯­ä¹‰ç›¸ä¼¼åº¦
    query_emb = _embedder.encode_queries([query])['dense'][0]
    doc_embs = _embedder.encode_documents(texts)['dense']

    query_emb = np.array(query_emb)
    doc_embs = np.array(doc_embs)

    dot_products = doc_embs @ query_emb
    query_norm = np.linalg.norm(query_emb)
    doc_norms = np.linalg.norm(doc_embs, axis=1)
    cosine_scores = dot_products / (doc_norms * query_norm + 1e-8)

    # 5. èåˆåŸå§‹å¾—åˆ†ä¸è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œ rerank
    reranked = []
    for i, hit in enumerate(hits):
        orig_score = getattr(hit, 'score', 0.0)  # é˜²æ­¢æŠ¥é”™
        semantic_score = cosine_scores[i]
        final_score = alpha * orig_score + (1 - alpha) * semantic_score
        reranked.append((final_score, hit.entity['text']))

    reranked.sort(key=lambda x: x[0], reverse=True)

    return [text for _, text in reranked]


# ------------------ LLMç”Ÿæˆæ¨¡å— ------------------
def generate_response(_tokenizer, _model, context, query, enable_thinking=True):
    """ç”Ÿæˆå›ç­”ï¼Œenable_thinking æ§åˆ¶æ˜¯å¦è¾“å‡º <think> æ ‡ç­¾"""
    SYSTEM_PROMPT = "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š"
    USER_PROMPT = f"""
    <context>
    {context}
    </context>
    <question>
    {query}
    </question>
    """

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": USER_PROMPT}
    ]
    text = _tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=enable_thinking
    )

    inputs = _tokenizer([text], return_tensors="pt").to(_model.device)
    outputs = _model.generate(
        **inputs,
        max_new_tokens=512
    )
    response = _tokenizer.decode(
        outputs[0][len(inputs.input_ids[0]):],
        skip_special_tokens=True
    ).strip("\n")
    return response


def continue_response(_tokenizer, _model, query, previous_answer):
    """
    ç»­å†™å›ç­”ï¼Œä¸å¸¦ <think> æ ‡ç­¾ï¼Œç”Ÿæˆå®Œæ•´è¿è´¯ç­”æ¡ˆ
    """
    SYSTEM_PROMPT = (
        "è¯·æ ¹æ®ç”¨æˆ·æé—®å’Œä¹‹å‰çš„å›ç­”ï¼Œç”Ÿæˆä¸€ä¸ªæ›´å®Œæ•´ã€æ›´è¿è´¯ã€"
        "é£æ ¼ç»Ÿä¸€çš„å®Œæ•´ç­”æ¡ˆã€‚è¯·å‹¿è¾“å‡ºä»»ä½• <think> å†…å®¹ã€‚"
    )
    USER_PROMPT = f"""
    <question>
    {query}
    </question>
    <previous_answer>
    {previous_answer}
    </previous_answer>
    """

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": USER_PROMPT}
    ]
    text = _tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False  # ç»­å†™æ—¶å…³é—­ thinking æ¨¡å¼
    )

    inputs = _tokenizer([text], return_tensors="pt").to(_model.device)
    outputs = _model.generate(
        **inputs,
        max_new_tokens=512
    )
    response = _tokenizer.decode(
        outputs[0][len(inputs.input_ids[0]):],
        skip_special_tokens=True
    ).strip("\n")
    return response


# ------------------ Streamlitç•Œé¢ ------------------
st.title("RAG+Qwen æ™ºèƒ½é—®ç­”ç³»ç»Ÿ ğŸ’¬")

# åˆå§‹åŒ–ç»„ä»¶ï¼ˆå‡è®¾ä½ å·²æœ‰ init_componentsï¼‰
try:
    client, embedder, tokenizer, model = init_components()
except Exception as e:
    st.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    st.stop()

# æ¸…ç©ºèŠå¤©æŒ‰é’®
if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯"):
    st.session_state.messages = []
    st.rerun()

# åˆå§‹åŒ–èŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘å°†ç»“åˆçŸ¥è¯†åº“ä¸ºæ‚¨è§£ç­”ã€‚"}]

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ä¸‹è½½èŠå¤©è®°å½•æŒ‰é’®
chat_text = "\n\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])
st.download_button("ğŸ’¬ ä¸‹è½½èŠå¤©è®°å½•", data=chat_text, file_name="chat_history.txt")

# å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆæ–°é—®é¢˜ï¼‰
if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    with st.spinner("ç¬¬ 1 æ­¥ï¼šæ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."):
        try:
            context = hybrid_search(client, embedder, user_input)
            context_str = "\n".join(context[:3])  # å–å‰3æ¡ç»“æœ

            with st.expander("ğŸ“š å‚è€ƒä¸Šä¸‹æ–‡ï¼ˆæ¥è‡ªçŸ¥è¯†åº“ï¼‰", expanded=False):
                for i, chunk in enumerate(context[:3]):
                    st.markdown(f"**ç‰‡æ®µ {i+1}:**\n{chunk}")

            with st.spinner("ç¬¬ 2 æ­¥ï¼šæ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”..."):
                # æ–°é—®é¢˜é¦–æ¬¡å›ç­”ï¼Œå¯ç”¨ thinking æ¨¡å¼ï¼Œå¸¦ <think>
                response = generate_response(tokenizer, model, context_str, user_input, enable_thinking=True)

            st.session_state.messages.append({"role": "assistant", "content": response})
            st.chat_message("assistant").write(response)

        except Exception as e:
            st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            st.session_state.messages.append({"role": "assistant", "content": f"å‡ºé”™äº†: {str(e)}"})

# ç»­å†™æŒ‰é’®ï¼ˆå­˜åœ¨ä¸Šä¸€æ¬¡åŠ©æ‰‹å›ç­”æ—¶å‡ºç°ï¼‰
if st.session_state.messages and st.session_state.messages[-1]["role"] == "assistant":
    if st.button("ç»§ç»­ç”Ÿæˆæ›´å¤šå†…å®¹"):
        # æ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”
        last_assistant_answer = None
        last_user_question = None

        for msg in reversed(st.session_state.messages):
            if msg["role"] == "assistant" and last_assistant_answer is None:
                last_assistant_answer = msg["content"]
            elif msg["role"] == "user" and last_user_question is None:
                last_user_question = msg["content"]
            if last_assistant_answer and last_user_question:
                break

        if last_user_question and last_assistant_answer:
            with st.spinner("ç»§ç»­ç”Ÿæˆä¸­..."):
                continued_answer = continue_response(tokenizer, model, last_user_question, last_assistant_answer)

            # ç”¨ç»­å†™å†…å®¹æ›¿æ¢æœ€åä¸€æ¬¡åŠ©æ‰‹æ¶ˆæ¯ï¼Œä¿è¯è¿è´¯
            st.session_state.messages[-1]["content"] = continued_answer
            st.rerun()